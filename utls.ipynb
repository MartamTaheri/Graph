{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csgraph\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_random_walk(adj):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv = np.power(row_sum, -1.0).flatten()\n",
    "    d_mat = sp.diags(d_inv)\n",
    "    return (d_mat.dot(adj)).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(mx, norm):\n",
    "    \"\"\"Laplacian-normalize sparse matrix\"\"\"\n",
    "    assert (all(len(row) == len(mx) for row in mx)), \"Input should be a square matrix\"\n",
    "\n",
    "    return csgraph.laplacian(adj, normed=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_as_Adj_matrix(Alledge,features):\n",
    "    import scipy.sparse as sp\n",
    "    relation_matrix = np.zeros((len(features),len(features)))\n",
    "    for i, j in np.array(Alledge):\n",
    "        lnc, mi = int(i), int(j)\n",
    "        relation_matrix[lnc, mi] = 1\n",
    "    Adj = sp.csr_matrix(relation_matrix, dtype=np.float32)\n",
    "    return Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(edgelist,node_features,node_labels):\n",
    "    features = sp.csr_matrix(node_features, dtype=np.float32)\n",
    "    # features = normalize(features)\n",
    "    idx_train = range(500)\n",
    "    idx_val = range(500, 660)\n",
    "    idx_test = range(660, int(node_features.shape[0]))  \n",
    "    features = torch.FloatTensor(np.array(features.todense()))  \n",
    "    labels = torch.LongTensor(np.array(node_labels))\n",
    "    adj = load_file_as_Adj_matrix(edgelist,node_features)\n",
    "    # adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    # adj = sparse_mx_to_torch_sparse_tensor(adj)    \n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data1(path=\"../data\", dataset=\"cora\"):\n",
    "    \"\"\"\n",
    "    ind.[:dataset].x     => the feature vectors of the training instances (scipy.sparse.csr.csr_matrix)\n",
    "    ind.[:dataset].y     => the one-hot labels of the labeled training instances (numpy.ndarray)\n",
    "    ind.[:dataset].allx  => the feature vectors of both labeled and unlabeled training instances (csr_matrix)\n",
    "    ind.[:dataset].ally  => the labels for instances in ind.dataset_str.allx (numpy.ndarray)\n",
    "    ind.[:dataset].graph => the dict in the format {index: [index of neighbor nodes]} (collections.defaultdict)\n",
    "    ind.[:dataset].tx => the feature vectors of the test instances (scipy.sparse.csr.csr_matrix)\n",
    "    ind.[:dataset].ty => the one-hot labels of the test instances (numpy.ndarray)\n",
    "    ind.[:dataset].test.index => indices of test instances in graph, for the inductive setting\n",
    "    \"\"\"\n",
    "    print(\"Upload {} dataset.\".format(dataset))\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        with open(\"{}/ind.{}.{}\".format(path, dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "\n",
    "    test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(path, dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Citeseer dataset contains some isolated nodes in the graph\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    print(\"| # of nodes : {}\".format(adj.shape[0]))\n",
    "    print(\"| # of edges : {}\".format(adj.sum().sum() / 2))\n",
    "\n",
    "    #features = normalize(features)\n",
    "    print(\"| # of features : {}\".format(features.shape[1]))\n",
    "    print(\"| # of clases   : {}\".format(ally.shape[1]))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    sparse_mx = adj.tocoo().astype(np.float32)\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        save_label = np.where(labels)[1]\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "\n",
    "    print(\"| # of train set : {}\".format(len(idx_train)))\n",
    "    print(\"| # of val set   : {}\".format(len(idx_val)))\n",
    "    print(\"| # of test set  : {}\".format(len(idx_test)))\n",
    "\n",
    "    idx_train, idx_val, idx_test = list(map(lambda x: torch.LongTensor(x), [idx_train, idx_val, idx_test]))\n",
    "\n",
    "    def missing_elements(L):\n",
    "        start, end = L[0], L[-1]\n",
    "        return sorted(set(range(start, end + 1)).difference(L))\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        L = np.sort(idx_test)\n",
    "        missing = missing_elements(L)\n",
    "\n",
    "        for element in missing:\n",
    "            save_label = np.insert(save_label, element, 0)\n",
    "\n",
    "        labels = torch.LongTensor(save_label)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def graph_decompose(adj,graph_name,k,metis_p,strategy=\"edge\"):\n",
    "    '''\n",
    "    Input:\n",
    "        adj:the adjacency matrix of original graph\n",
    "        graph_name:\"cora\",\"citeseer\",\"pubmed\"\n",
    "        k:decompose into k subgraphs\n",
    "        metis_p:\"no_skeleton\",\"all_skeleton\",\"number\" (depending on metis preprocessing) \n",
    "        strategy:\"edge\" (for edge_decomposition),\"node\" (for node_decomposition)\n",
    "    Output:\n",
    "        the decomposed subgraphs\n",
    "    '''\n",
    "    print(\"Skeleton:\",metis_p)\n",
    "    print(\"Strategy:\",strategy)\n",
    "    g,g_rest,edges_rest,gs=get_graph_skeleton(adj,graph_name,k,metis_p)\n",
    "    gs=allocate_edges(g_rest,edges_rest, gs, strategy)\n",
    "       \n",
    "    re=[]       \n",
    "   \n",
    "    #print the info of nodes and edges of subgraphs \n",
    "    edge_num_avg=0\n",
    "    compo_num_avg=0\n",
    "    print(\"Subgraph information:\")\n",
    "    for i in range(k):\n",
    "        nodes_num=gs[i].number_of_nodes()\n",
    "        edge_num=gs[i].number_of_edges()\n",
    "        compo_num=nx.number_connected_components(gs[i])\n",
    "        print(\"\\t\",nodes_num,edge_num,compo_num)\n",
    "        edge_num_avg+=edge_num\n",
    "        compo_num_avg+=compo_num\n",
    "        re.append(nx.to_scipy_sparse_matrix(gs[i])) \n",
    "        \n",
    "    #check the shared edge number in all subgrqphs\n",
    "    edge_share=set(sort_edge(gs[0].edges()))\n",
    "    for i in range(k):        \n",
    "        edge_share&=set(sort_edge(gs[i].edges()))\n",
    "        \n",
    "    print(\"\\tShared edge number is: %d\"%len(edge_share))\n",
    "    print(\"\\tAverage edge number:\",edge_num_avg/k) \n",
    "    print(\"\\tAverage connected component number:\",compo_num_avg/k)\n",
    "    print(\"\\n\"+\"-\"*70+\"\\n\")\n",
    "    return re\n",
    "\n",
    "def sort_edge(edges):\n",
    "    edges=list(edges)\n",
    "    for i in range(len(edges)):\n",
    "        u=edges[i][0]\n",
    "        v=edges[i][1]\n",
    "        if u > v:\n",
    "            edges[i]=(v,u)\n",
    "    return edges\n",
    "\n",
    "def get_graph_skeleton(adj,graph_name,k,metis_p): \n",
    "    '''\n",
    "    Input:\n",
    "        adj:the adjacency matrix of original graph\n",
    "        graph_name:\"cora\",\"citeseer\",\"pubmed\"\n",
    "        k:decompose into k subgraphs\n",
    "        metis_p:\"no_skeleton\",\"all_skeleton\",\"k\" \n",
    "    Output:\n",
    "        g:the original graph\n",
    "        g_rest:the rest graph\n",
    "        edges_rest:the rest edges\n",
    "        gs:the skeleton of the graph for every subgraph\n",
    "    '''\n",
    "    g=nx.from_numpy_matrix(adj.todense())\n",
    "    num_nodes=g.number_of_nodes()\n",
    "    print(\"Original nodes number:\",num_nodes)\n",
    "    num_edges=g.number_of_edges()\n",
    "    print(\"Original edges number:\",num_edges)  \n",
    "    print(\"Original connected components number:\",nx.number_connected_components(g),\"\\n\")    \n",
    "    \n",
    "    g_dic=dict()\n",
    "    \n",
    "    for v,nb in g.adjacency():\n",
    "        g_dic[v]=[u[0] for u in nb.items()] \n",
    "            \n",
    "    #initialize all the subgrapgs, add the nodes\n",
    "    gs=[nx.Graph() for i in range(k)]\n",
    "    for i in range(k):\n",
    "        gs[i].add_nodes_from([i for i in range(num_nodes)])\n",
    "    \n",
    "    if metis_p==\"no_skeleton\":\n",
    "        #no skeleton\n",
    "        g_rest=g\n",
    "        edges_rest=list(g_rest.edges())\n",
    "    else:    \n",
    "        if metis_p==\"all_skeleton\":\n",
    "            #doesn't use metis to cut any edge\n",
    "            graph_cut=g\n",
    "        else:\n",
    "            #read the cluster info from file\n",
    "            f=open(\"metis_file/\"+graph_name+\".graph.part.%s\"%metis_p,'r')\n",
    "            cluster=dict()  \n",
    "            i=0\n",
    "            for lines in f:\n",
    "                cluster[i]=eval(lines.strip(\"\\n\"))\n",
    "                i+=1\n",
    "           \n",
    "            #get the graph cut by Metis    \n",
    "            graph_cut=nx.Graph()\n",
    "            graph_cut.add_nodes_from([i for i in range(num_nodes)])  \n",
    "            \n",
    "            for v in range(num_nodes):\n",
    "                v_class=cluster[v]\n",
    "                for u in g_dic[v]:\n",
    "                    if cluster[u]==v_class:\n",
    "                        graph_cut.add_edge(v,u)\n",
    "            \n",
    "        subgs=list(nx.connected_component_subgraphs(graph_cut))\n",
    "        print(\"After Metis,connected component number:\",len(subgs))\n",
    "        \n",
    "                \n",
    "        #add the edges of spanning tree, get the skeleton\n",
    "        for i in range(k):\n",
    "            for subg in subgs:\n",
    "                T=get_spanning_tree(subg)\n",
    "                gs[i].add_edges_from(T)\n",
    "        \n",
    "        #get the rest graph including all the edges except the shared egdes of spanning trees\n",
    "        edge_set_share=set(sort_edge(gs[0].edges()))\n",
    "        for i in range(k):\n",
    "            edge_set_share&=set(sort_edge(gs[i].edges()))\n",
    "        edge_set_total=set(sort_edge(g.edges()))\n",
    "        edge_set_rest=edge_set_total-edge_set_share   \n",
    "        edges_rest=list(edge_set_rest)\n",
    "        g_rest=nx.Graph()\n",
    "        g_rest.add_nodes_from([i for i in range(num_nodes)])\n",
    "        g_rest.add_edges_from(edges_rest)\n",
    "       \n",
    "          \n",
    "    #print the info of nodes and edges of subgraphs\n",
    "    print(\"Skeleton information:\")\n",
    "    for i in range(k):\n",
    "        print(\"\\t\",gs[i].number_of_nodes(),gs[i].number_of_edges(),nx.number_connected_components(gs[i])) \n",
    "        \n",
    "    edge_set_share=set(sort_edge(gs[0].edges()))\n",
    "    for i in range(k):\n",
    "        edge_set_share&=set(sort_edge(gs[i].edges()))\n",
    "    print(\"\\tShared edge number is: %d\\n\"%len(edge_set_share))\n",
    "    \n",
    "    return g,g_rest,edges_rest,gs\n",
    "\n",
    "def get_spanning_tree(g):\n",
    "    '''\n",
    "    Input:Graph\n",
    "    Output:list of the edges in spanning tree\n",
    "    '''\n",
    "    g_dic=dict()\n",
    "    for v,nb in g.adjacency():\n",
    "        g_dic[v]=[u[0] for u in nb.items()]\n",
    "        np.random.shuffle(g_dic[v])\n",
    "    flag_dic=dict()\n",
    "    if g.number_of_nodes() ==1:\n",
    "        return []\n",
    "    gnodes=np.array(g.nodes)\n",
    "    np.random.shuffle(gnodes)\n",
    "    \n",
    "    for v in gnodes:\n",
    "        flag_dic[v]=0\n",
    "    \n",
    "    current_path=[]\n",
    "    \n",
    "    def dfs(u):\n",
    "        stack=[u]\n",
    "        current_node=u\n",
    "        flag_dic[u]=1\n",
    "        while len(current_path)!=(len(gnodes)-1):\n",
    "            pop_flag=1\n",
    "            for v in g_dic[current_node]:\n",
    "                if flag_dic[v]==0:\n",
    "                    flag_dic[v]=1\n",
    "                    current_path.append((current_node,v))  \n",
    "                    stack.append(v)\n",
    "                    current_node=v\n",
    "                    pop_flag=0\n",
    "                    break\n",
    "            if pop_flag:\n",
    "                stack.pop()\n",
    "                current_node=stack[-1]     \n",
    "    dfs(gnodes[0])        \n",
    "    return current_path\n",
    "\n",
    "def allocate_edges(g_rest,edges_rest, gs, strategy):\n",
    "    '''\n",
    "    Input:\n",
    "        g_rest:the rest graph\n",
    "        edges_rest:the rest edges\n",
    "        gs:the skeleton of the graph for every subgraph\n",
    "        strategy:\"edge\" (for edge_decomposition),\"node\" (for node_decomposition)\n",
    "    Output:\n",
    "        the decomposed graphs after allocating rest edges\n",
    "    '''\n",
    "    k=len(gs)\n",
    "    if strategy==\"edge\":  \n",
    "        print(\"Allocate the rest edges randomly and averagely.\")\n",
    "        np.random.shuffle(edges_rest)\n",
    "        t=int(len(edges_rest)/k)\n",
    "        \n",
    "        #add edges\n",
    "        for i in range(k):       \n",
    "            if i == k-1:\n",
    "                gs[i].add_edges_from(edges_rest[t*i:])\n",
    "            else:\n",
    "                gs[i].add_edges_from(edges_rest[t*i:t*(i+1)])        \n",
    "        return gs\n",
    "    \n",
    "    elif strategy==\"node\":\n",
    "        print(\"Allocate the edges of each nodes randomly and averagely.\")\n",
    "        g_dic=dict()    \n",
    "        for v,nb in g_rest.adjacency():\n",
    "            g_dic[v]=[u[0] for u in nb.items()]\n",
    "            np.random.shuffle(g_dic[v])\n",
    "        \n",
    "        def sample_neighbors(nb_ls,k):\n",
    "            np.random.shuffle(nb_ls)\n",
    "            ans=[]\n",
    "            for i in range(k):\n",
    "                ans.append([])\n",
    "            if len(nb_ls) == 0:\n",
    "                return ans\n",
    "            if len(nb_ls) > k:\n",
    "                t=int(len(nb_ls)/k)\n",
    "                for i in range(k):\n",
    "                    ans[i]+=nb_ls[i*t:(i+1)*t]\n",
    "                nb_ls=nb_ls[k*t:]\n",
    "            '''\n",
    "            if len(nb_ls)>0:\n",
    "                for i in range(k):\n",
    "                    ans[i].append(nb_ls[i%len(nb_ls)])\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            if len(nb_ls)>0:\n",
    "                for i in range(len(nb_ls)):\n",
    "                    ans[i].append(nb_ls[i])\n",
    "            \n",
    "            np.random.shuffle(ans)\n",
    "            return ans\n",
    "        \n",
    "        #add edges\n",
    "        for v,nb in g_dic.items():\n",
    "            ls=np.array(sample_neighbors(nb,k))\n",
    "            for i in range(k):\n",
    "                gs[i].add_edges_from([(v,j) for j in ls[i]])\n",
    "        \n",
    "        return gs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
